{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/root/.cache/pypoetry/virtualenvs/data-transformations-_a9Tn7A7-py3.9/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/26 00:54:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/26 00:54:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|Translated by Dav...|\n",
      "|                    |\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "|almost no restric...|\n",
      "|re-use it under t...|\n",
      "|with this eBook o...|\n",
      "|                    |\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import logging \n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from transformations.nlp import SparkJob\n",
    "\n",
    "LOG_FILENAME = 'solana.log'\n",
    "logging.basicConfig(filename=LOG_FILENAME, level=logging.INFO)\n",
    "\n",
    "job = SparkJob()\n",
    "df = job.preprocessing_words()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|Translated by Dav...|\n",
      "|                    |\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "|almost no restric...|\n",
      "|re-use it under t...|\n",
      "|with this eBook o...|\n",
      "|                    |\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = job.preprocessing_words()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|[T, h, e,  , P, r...|\n",
      "|[T, r, a, n, s, l...|\n",
      "|                  []|\n",
      "|                  []|\n",
      "|[T, h, i, s,  , e...|\n",
      "|[a, l, m, o, s, t...|\n",
      "|[r, e, -, u, s, e...|\n",
      "|[w, i, t, h,  , t...|\n",
      "|                  []|\n",
      "|                  []|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = df.select(split('value', pattern='').alias('value'))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=['T', 'h', 'e', ' ', 'P', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'G', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'E', 'B', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'M', 'e', 't', 'a', 'm', 'o', 'r', 'p', 'h', 'o', 's', 'i', 's', ',', ' ', 'b', 'y', ' ', 'F', 'r', 'a', 'n', 'z', ' ', 'K', 'a', 'f', 'k', 'a', ''], grams=['T h', 'h e', 'e  ', '  P', 'P r', 'r o', 'o j', 'j e', 'e c', 'c t', 't  ', '  G', 'G u', 'u t', 't e', 'e n', 'n b', 'b e', 'e r', 'r g', 'g  ', '  E', 'E B', 'B o', 'o o', 'o k', 'k  ', '  o', 'o f', 'f  ', '  M', 'M e', 'e t', 't a', 'a m', 'm o', 'o r', 'r p', 'p h', 'h o', 'o s', 's i', 'i s', 's ,', ',  ', '  b', 'b y', 'y  ', '  F', 'F r', 'r a', 'a n', 'n z', 'z  ', '  K', 'K a', 'a f', 'f k', 'k a', 'a ']),\n",
       " Row(value=['T', 'r', 'a', 'n', 's', 'l', 'a', 't', 'e', 'd', ' ', 'b', 'y', ' ', 'D', 'a', 'v', 'i', 'd', ' ', 'W', 'y', 'l', 'l', 'i', 'e', '.', ''], grams=['T r', 'r a', 'a n', 'n s', 's l', 'l a', 'a t', 't e', 'e d', 'd  ', '  b', 'b y', 'y  ', '  D', 'D a', 'a v', 'v i', 'i d', 'd  ', '  W', 'W y', 'y l', 'l l', 'l i', 'i e', 'e .', '. ']),\n",
       " Row(value=[''], grams=[]),\n",
       " Row(value=[''], grams=[]),\n",
       " Row(value=['T', 'h', 'i', 's', ' ', 'e', 'B', 'o', 'o', 'k', ' ', 'i', 's', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'u', 's', 'e', ' ', 'o', 'f', ' ', 'a', 'n', 'y', 'o', 'n', 'e', ' ', 'a', 'n', 'y', 'w', 'h', 'e', 'r', 'e', ' ', 'a', 't', ' ', 'n', 'o', ' ', 'c', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'w', 'i', 't', 'h', ''], grams=['T h', 'h i', 'i s', 's  ', '  e', 'e B', 'B o', 'o o', 'o k', 'k  ', '  i', 'i s', 's  ', '  f', 'f o', 'o r', 'r  ', '  t', 't h', 'h e', 'e  ', '  u', 'u s', 's e', 'e  ', '  o', 'o f', 'f  ', '  a', 'a n', 'n y', 'y o', 'o n', 'n e', 'e  ', '  a', 'a n', 'n y', 'y w', 'w h', 'h e', 'e r', 'r e', 'e  ', '  a', 'a t', 't  ', '  n', 'n o', 'o  ', '  c', 'c o', 'o s', 's t', 't  ', '  a', 'a n', 'n d', 'd  ', '  w', 'w i', 'i t', 't h', 'h ']),\n",
       " Row(value=['a', 'l', 'm', 'o', 's', 't', ' ', 'n', 'o', ' ', 'r', 'e', 's', 't', 'r', 'i', 'c', 't', 'i', 'o', 'n', 's', ' ', 'w', 'h', 'a', 't', 's', 'o', 'e', 'v', 'e', 'r', '.', ' ', ' ', 'Y', 'o', 'u', ' ', 'm', 'a', 'y', ' ', 'c', 'o', 'p', 'y', ' ', 'i', 't', ',', ' ', 'g', 'i', 'v', 'e', ' ', 'i', 't', ' ', 'a', 'w', 'a', 'y', ' ', 'o', 'r', ''], grams=['a l', 'l m', 'm o', 'o s', 's t', 't  ', '  n', 'n o', 'o  ', '  r', 'r e', 'e s', 's t', 't r', 'r i', 'i c', 'c t', 't i', 'i o', 'o n', 'n s', 's  ', '  w', 'w h', 'h a', 'a t', 't s', 's o', 'o e', 'e v', 'v e', 'e r', 'r .', '.  ', '   ', '  Y', 'Y o', 'o u', 'u  ', '  m', 'm a', 'a y', 'y  ', '  c', 'c o', 'o p', 'p y', 'y  ', '  i', 'i t', 't ,', ',  ', '  g', 'g i', 'i v', 'v e', 'e  ', '  i', 'i t', 't  ', '  a', 'a w', 'w a', 'a y', 'y  ', '  o', 'o r', 'r ']),\n",
       " Row(value=['r', 'e', '-', 'u', 's', 'e', ' ', 'i', 't', ' ', 'u', 'n', 'd', 'e', 'r', ' ', 't', 'h', 'e', ' ', 't', 'e', 'r', 'm', 's', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'P', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'G', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'L', 'i', 'c', 'e', 'n', 's', 'e', ' ', 'i', 'n', 'c', 'l', 'u', 'd', 'e', 'd', ''], grams=['r e', 'e -', '- u', 'u s', 's e', 'e  ', '  i', 'i t', 't  ', '  u', 'u n', 'n d', 'd e', 'e r', 'r  ', '  t', 't h', 'h e', 'e  ', '  t', 't e', 'e r', 'r m', 'm s', 's  ', '  o', 'o f', 'f  ', '  t', 't h', 'h e', 'e  ', '  P', 'P r', 'r o', 'o j', 'j e', 'e c', 'c t', 't  ', '  G', 'G u', 'u t', 't e', 'e n', 'n b', 'b e', 'e r', 'r g', 'g  ', '  L', 'L i', 'i c', 'c e', 'e n', 'n s', 's e', 'e  ', '  i', 'i n', 'n c', 'c l', 'l u', 'u d', 'd e', 'e d', 'd ']),\n",
       " Row(value=['w', 'i', 't', 'h', ' ', 't', 'h', 'i', 's', ' ', 'e', 'B', 'o', 'o', 'k', ' ', 'o', 'r', ' ', 'o', 'n', 'l', 'i', 'n', 'e', ' ', 'a', 't', ' ', 'w', 'w', 'w', '.', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', '.', 'n', 'e', 't', ''], grams=['w i', 'i t', 't h', 'h  ', '  t', 't h', 'h i', 'i s', 's  ', '  e', 'e B', 'B o', 'o o', 'o k', 'k  ', '  o', 'o r', 'r  ', '  o', 'o n', 'n l', 'l i', 'i n', 'n e', 'e  ', '  a', 'a t', 't  ', '  w', 'w w', 'w w', 'w .', '. g', 'g u', 'u t', 't e', 'e n', 'n b', 'b e', 'e r', 'r g', 'g .', '. n', 'n e', 'e t', 't ']),\n",
       " Row(value=[''], grams=[]),\n",
       " Row(value=[''], grams=[])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram(n=2, inputCol='value', outputCol='grams')\n",
    "ngram.transform(df).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02a1baf611809a3301ad48a965f309c6748f1b74e4cd6aab3f1b1057faddf914"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('data-transformations-_a9Tn7A7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
